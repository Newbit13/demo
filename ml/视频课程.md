学习笔记，以视频集数为标题，总结每集的收获和知识点

[黑马最热门的人工智能机器学习及机器视觉终于来了！从入门到精通（附赠课件资料+笔记](https://www.bilibili.com/video/BV1wy4y1T78o)
这个视频TensorFlow用的是1.x的版本，从网上资料来看，目前2.x版本中已经废弃了很多1.x的api，并且操作上更加简洁，所以这个视频我先暂时不看了

# p2学习笔记
## 深度学习与机器学习的区别
特征提取方面：
    机器学习需要自己提取特征，这需要大量领域专业知识；
    深度学习（通常由多个层组成），通过大量数据训练自动得到模型，适用于难提取特征的图像，语音，自然语言领域（应用场景）；

数据量方面：
    深度学习一般数据量越大结论越准确，而机器学习在一定数据量后达到一个相对稳定的准确率，并不会随着后面数据量的增大而表现更佳。
    深度学习需要的算力也比较大

算法代表
    机器学习：朴素贝叶斯、决策树
    深度学习：神经网络



个人总结：在容易提取特征的领域，采用机器学习的方法，通过手动提取特征，在效率和效果方面应该是表现不错的。深度学习需要的数据量应该较大，提取模型的成本也比较大。所以不能说 深度学习 就比 机器学习 优

深度学习框架：github start关注度历年来前二分别是TensorFlow，Caffe 



[[中英字幕]吴恩达机器学习系列课程](https://www.bilibili.com/video/BV164411b7dx)

笔记
# p1
机器学习应用领域广泛，如自动驾驶，基因测序，研究人脑思考方式，搜索引擎推荐，图片分类，识别垃圾邮件等

# p2
介绍一般对于机器学习的定义，给一个任务T,通过经验E,得到性能度量P(我认为算做正确的概率)
这课程会教很多不同类型的学习算法，最主要的两类是 监督学习(supervised learning)和无监督学习（其他的经常听到的有，强化学习，推荐系统等）
简单的说 监督学习是我们让机器学会处理某一件事；无监督学习是我们让机器自己学习

# p3 
回归问题，值是连续的
分类问题，值是离散的
总结：通过值去判断要用把问题归为哪一类（进而选择合适的算法）
这些属于监督学习的范围

# p4
将无监督学习
中监督学习的例子中，我们会被告知什么是良性肿瘤，什么是恶性的；
现在我们拿到一推数据，没人告诉我们这些数据的意义，这时候就属于用到无监督学习的范围了
提到 
聚类算法，将数据归为一个簇一个簇的（应用例子：谷歌新闻：每天收集大量新闻并分为各种专题；给一推基因数据让其分类，类型我们事先未知；根据邮件判断哪些人可能是朋友；通过客户数据找到不同的细分市场）
鸡尾酒会算法（应用例子：耳机降噪），实际上一行算法：[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
svd：singular value decomposition 奇异值分解（线性代数常规函数）


提到Octave、Matlab这样的软件
作者推荐使用Octave，在硅谷很多人会先用Octave建立软件原型，因为在Octave中实现这些学习算法的速度很快，在这个软件中，例如svd函数也内置了，不然如果自己用c++或者java库我们需要自己写很多代码，当在软件里可以运行时，我们再将其迁移到其他环境（java，c++等）下，这样效率更高（根据弹幕有歧义，说现在python可以很方便的现实功能）

# p5
了解监督学习的过程
1. Traning Set训练集 --输入到>> Learning Algorithm学习算法 --得到>> h(hypotheis)假设函数
2. 输入值通过->h->输出预测值
# p6 代价函数
如果假设函数是这个形式： hθ(x) = θ0 + θ1x;
通过找到最小的值：minimize_θ0θ0  J(θ0,θ0) =  (1/2m) * 0Σi=m (h(x^(i)) - y^(i))^2的平方差和来确定参数值 (这个公式算我手打的...可能不好看，我用_标识后面的符号在底部。。。)

这个1/2m是为了开导时抵消掉平方，对优化结果来说无影响（弹幕提供）
（微积分有点忘了，但是这里感悟是用积分方程可以减低计算量）
微积分有公式的，比如基本的导数公式中 x^u = u*x^(u-1)

J(θ0,θ1) 在这里就是代价函数Cost function，或者叫做lost function。这里这个具体的函数叫做squared error function

这个squared error function对大多数回归问题是个合理的选择（常用）

# p7 代价函数2
作者简化了一下函数：hθ(x) = θ1x;然后将x值代进代价函数中，把θ1作为变量
画出了θ1与结果的图像变化图（可以看到变化的规律，嗯...没什么，算是加深一下对公式作用的理解吧）

# p8 代价函数3
假设函数换回这个形式： hθ(x) = θ0 + θ1x;
然后画图

提到 contour plots等高线图
如果是将x带入，那么由θ0，θ1，该代价函数的值分别坐标x,y,z轴，由此组成的立体图像会呈现一个碗状，在平面中我们可以用等高线去表示

从目前简单的例子可以体会到如何一步步寻找最小值（目前使用眼睛观察的），这让我想起梯度下降，动量等模糊概念，这些概念的存在意义就是为了加速得到最小值。而不是先遍历所有可能（遍历所有的值，在这就是画出三维立体图）

多维度参数想可视化就没那么简单了

# p9 梯度下降
gradient descent梯度下降，是一个很常用的算法。不仅可用在最小化线性回归的代价函数上

按照两个参数来举例：θ0、θ1
大概思路：
首先给θ0、θ1分别一个初始值，随便什么值都行。例如θ0 = 0、θ1 = 0
不断改变θ0、θ1，使得 代价函数J(θ0、θ1)变小，直到找到最小值或局部最小值（这很明显跟初始位置及“山峰”形状有关）

梯度下降算法：
重复更新参数θj直至收敛
repeat until convergence{
    θj :=θj - α(∂/∂θj)J(θ0、θ1)  (simultaneously update j=0 and j=1)
}
在这就是同时更新θ0和θ1

:= 表示赋值（如果单纯用=，在这里是断言，会得到true或false）
α 学习率,是一个数值，在梯度下降中，意味着我们迈出多大的步子，值大意味着梯度下降很迅速
(∂/∂θj)J(θ0、θ1) 是一个导数项（没见过偏导数的话看一下下面提供的参考资料）  

(∂/∂x)f(x,y)，表示函数f(x,y)对变量x的偏导
(∂/∂y)f(x,y)，表示函数f(x,y)对变量y的偏导

参考资料
[第二节 偏导数](http://netedu.xauat.edu.cn/jpkc/netedu/jpkc/gdsx/homepage/5jxsd/51/513/5308/530802.htm)
在资料中可以了解到，求偏导数跟求导数方法一样，先把其他变量看成常量，再用求一元函数的导数的方法去求导

# p10 梯度下降算法里那个导数项的相关内容
derivative 导数
导数和偏导数是什么？（惨，大学数学给忘了）这里我百度了一下 [偏导数、微分、以及导数到底有什么关系和区别？](https://www.zhihu.com/question/265021971)


一个函数的导数，就是函数在图像上的斜率
偏导数，偏是局部的意思，一般是是指一个多元函数，其他变量都确定时，只对有剩余一个不确定的变量函数的导数。在几何意义上，就是一个点在任意确定的面上的切线的斜率。（想象一下，一个三维物体上的一个点，是不是有无数条切线，而二维平面图上，一条曲线是不是只有一条切线。所以求一条曲线的切线的斜率和求一个三维物体的其中一条切线的斜率就是导数和偏导数的区别）

在简化的代价函数J(θ1)中，对其θ1的偏导数就是 (∂/∂θ1)*J(θ1),这里是我个人理解的写法，一般不会这么表示，一般一元函数的导数写作J'(θ1)

α 学习率，如果太小，那么需要很多步才能到达局部低点；如果太大，可能会一次性越过最低点，导致无法收敛，也就是一直无法达到局部低点

随着θ1的更新，(∂/∂θ1)*J(θ1)作为斜率的值越来越小，所以θ1更新的幅度越来越小，最终我们会无限接近最低点。


## p1~p10 总结

对于一个线性的问题，
我们需要得到一个函数y=f(x),这样我们可以通过输入任意x得到答案y
这时候函数的形状可能有很多种，那么我们只能先假设，假设这个函数是直线，然后大概长这样：
f(x) = a + bx; a,b是一个固定的值，然后确定要用的损失函数，也就是代价函数，把a,b作为变量，通过h(a,b) 这个代价函数来确定a，b的值。

那么这里用什么形式的代价函数，用什么方法确定a，b的值，就是我们学习的重点了。
这几节课里我学习了squared error function在回归问题上作为代价函数效果不错，然后学习了梯度下降这种函数方法是如何来对这种代价函数进行处理的

所以目前我算是掌握了一个对于这种回归问题如何处理的整体思路。

    个人思考：
    目前感受到的就是算法、数学的魅力，但是智能还算不上，到不如说“自动”，就像你问计算机1+1，他跟你说等于2，那么分类，预测，只是相当于把1+1换成了更加复杂的东西，本质上还是计算方式

    那么神经网络，深度学习是不是毕竟接近我们印象中的智能呢？其实他也是一种算法，而我现在就在了解其具体的算法。这就是所谓的“人工智能”，跟人类的拥有复杂感情相比好像关系没那么明显或者说没有关系。但是仔细想想，大多数感情，感觉都是身体里的各种器官，激素的作用和变化引起“感觉”，“感觉”跟“人工智能”合在一起才是个完整的人。那么现在思路就更加清晰了，我在学的是“智能”，就是所谓的让计算机更加聪明的方法。所以重点在于“智能”，而不在于“人”上。“智能”目前是生物才具有的。所以研究神经网络的应用最接近“智能”了吧

# p11
这节课将代价函数带入并求偏导数的值（证明方法略）：

注意要同时更新θ0和θ1
repeat until convergence{
    θ0 :=θ0 - α * (1/m) * i=1 Σ m (hθ(xi)-yi)
    θ1 :=θ1 - α * (1/m) * i=1 Σ m (hθ(xi)-yi) * xi
}

xi代表训练集中第i个x的数据，m代表有训练集中有m个数据

用梯度下降法有个缺点就是得到的是局部底部，然而这里代价函数是线性回归方程，得到的三维图像是一个碗状的图形，这图形有个术语叫做convex function 凸函数

凸函数不会存在只得到局部最低点这个问题，它只有一个全局最优解

给这种梯度下降一个名字，叫做“Batch” Gradient Descent，Batch在这里表示每一步梯度都需要使用到所有的训练集数据（那个i=1 Σ m ）

# p12
提到矩阵与向量的概念,表达方式。（这个看视频，我不好表达）
矩阵，就像二维数组，维数就是行数*列数，书写形式有R^(2*2)

向量，就像一维数组，一个n*1的矩阵，可以写成R^n

# p13
矩阵加法，就是对应元素相加，并且得是相同结构的矩阵才能相加
矩阵和标量（就是实数）的乘法，就是每个对应的元素与标量相乘
# p14 矩阵乘法
先讲了个特殊例子， 矩阵跟向量的乘法：例中R^(3*2) x R^(2*1) 得到一个R^(3*1)的矩阵，那么看来要相乘的话，第一个矩阵的列数要跟第二个矩阵的行数相同

作者举了一个房子价格的例子，用矩阵跟向量的乘法，可以将不同房子大小这个变量发到矩阵中，配合价格函数构造出一个矩阵乘向量的模型（对这种计算的表达方式我觉得十分的miao），这样做的好处在于用更加简洁的公式表达了想法（相当于简化了代码,目前我具体感受不到）

# p15 矩阵与矩阵相乘
了解这个可以在线性回归中同时计算θ0和θ1而不用梯度下降函数（作者说的）

矩阵与矩阵相乘可以看作矩阵跟各个向量相乘的结果，拼起来（所以还是遵循一个规则：第一个矩阵的列数要跟第二个矩阵的行数相同）
通过举例多个预测价格函数（上一集将跟向量相乘时是举例一个价格函数），来体现矩阵与矩阵相乘的表达方式（miao啊又）

# p16
设计矩阵运算的一些特性：
不符合交换律（commutative）
符合结合律（associative）

单位矩阵（可以有多种维数的），只有Rii上的值为1，其他为0。表述为I(或者I_n*n),单位矩阵与矩阵相乘满足交换律  注意：交换后的单位矩阵的维数不一定跟原来的一致，切记

# p17
矩阵的逆运算(inverse operation)、矩阵的转置运算(transpose operation)

    A*A^-1 = I 这个A^-1就是矩阵A的逆矩阵。只有m*m的这种方阵才有逆矩阵 有一些特殊的方阵也是没用逆矩阵的，作者这里不讨论
不存在逆矩阵的矩阵有个专有名词叫做 奇异矩阵 singular matrix或者退化矩阵 degenerate matrix

A^T表示矩阵A的转置矩阵，具体表现为矩阵里的元素位置A_ij变成A_ji（第一行变成第一列，以此类推）

# p18
回到之前房屋价格预测的例子，如果这时候信息多了（特征量多了），原先只考虑房子大小跟价格的关系，现在可以考虑的因素有大小，楼层，房龄，房间数量。

那么重新定义一下：
n：特征数
x^(i)第i个训练集，是有一组特征值组成的向量
x^(i)_j 第i个训练集的第j个特征值（这里我写的格式不规范...）

代价函数变成了hθ(x1,x2,x3,x4) = θ0 + θ1x1 + θ2x2 + θ3x3 + θ4x4;(多元多变量线性回归假设)
通过构建 θ变量组成的向量 和x标量组成的向量，可以用来表示hθ(x1,x2,x3,x4)

# p19
    Hypothesis:hθ(x) = θ^T*x = θ0*x0 + θ1*x1 + θ2*x2 + ... + θn*xn  (这里θ和x分别是指向量)
    Parameter:θ0,θ1,...,θn (我们这里会用θ这个向量表示这一串变量)
    Cost function: J(θ0,θ1,...,θn) = (1/2m) * i=1Σm (hθ(x^(i)) - y^(i))^2   （同理J(θ0,θ1,...,θn)写成J(θ)）

n个特征时的梯度下降公式（见视频吧...）

# p20
梯度下降的实用技巧(提高效率，减少迭代速度)
技巧方法一：特征缩放
将特征缩小有利于更快的收敛，相反特征直接差距大会让等高线看起来向被压缩一样，并且梯度下降的路径会变得曲折，导致收敛速度慢。
我们需要在缩小后的各特征的值范围相接近，比如缩小后值都是-1~1之间

在特征缩放时，我们有时候还会进行 均值归一化 Mean normaliztion
xi变成xi-μi，这里μi表示xi的平均值

公式（xi-μi）/ si ,si是最大值-最小值得到的，叫做范围

# p21
提到通过观察迭代次数和代价函数的值组成的图像，来判断梯度下降方法是否有在正常工作。

如果不正常（例如代价函数的值随迭代次数增加而增加或者上下上下反复波动），意味着你应该使用较小的学习率α

我们会尝试使用不同的学习率α来观察效果

实际工作中，作者喜欢（觉得合适）每次3倍的涨幅对不同α进行尝试

# p22
作者想告诉我们一些可选择的特性，还有如何得到不同的学习算法（很不错的感觉），因为选择当选择合适的特征后，算法往往才会非常有效

提到多项式回归 polynomial regression,它让我们可以使用线性回归的方法来拟合复杂的函数甚至是非线性函数

将特征进行整合，可以得到有利于结果的更加有用的特征（关于这点作者举例如果你得到房子的长跟宽，我们不一定要把长跟宽分别作为特征，而将其相乘计算成房子面积这个特征会更有效）
通过定义新的特征，我们可能会得到更好的模型

接下来会了解到一些算法可以帮我们选择要用什么模型，要用什么多项式函数

# p23
作者提供一种正规方程来求得线性方程的最优值，而不是使用梯度下降算法


求最小化θ的公式（证明作者未给）：
θ = (X^T * X)^(-1) * X^T *y 其中X是一个m条数据 * n个特征构成的m * (n+1)的一个矩阵。y是由m条结果数据 构成的向量
这个公式能得到一个由m条θ数据 构成的向量

对比：
假如有m个训练样本，n个特征.
| 梯度下降 | 正规方程(Normal Equation) |
|---|---|
|需要选择学习率α|不用选择学习率|
|需要很多次迭代|不用使用迭代|
|即使n很大，也能很好的进行计算|当n很大时很慢,O(n^3)|


作者提到自己，一般小于10^4,现代计算机都能很快算出，所以会选正规方程，当`10^4<n<10^6`作者会犹豫下，然后选择正规方程，当大于100万，作者会选择梯度下降或者其他算法

# p24 正规方程在矩阵不可逆情况下的解决方法
作者提到出现逆矩阵的概率其实不大。
Octave软件里有两个函数可以求解矩阵的逆：pinv()和inv()
pinv是伪逆（pseudo-inverse），即使某个矩阵不可逆，用pinv也可以算出θ值

什么时候矩阵(X^T * X)会不可逆，

1. Redundant features(linearly dependent)
多余特征（线性依赖）
具体例子为，比如有两个特征，一个是表示房子的平方米，一个是房子的平方英寸，这两特征有线性关系，因为米跟英寸可以相互转换。
2. 太多特征（m<=n）
这时候可以选择删掉一些特征，或者使用regularization(正规化方法)（后面讨论）

# p26 Octave的基本操作
接下来是Octave语法的学习
Octave可以快速实现算法的原型（当你想构建大型的机器学习项目时），测验自己的想法是否有效，如果可行，咱们再可以用c++或者java等语言来实现，提高开发效率
我简单了解下语法就好，毕竟看弹幕提到目前是python在主导机器学习项目。
```Octave
% 表示注释
== 等于
~= 不等于
不正确时返回0，正确返回1
&& 与
|| 或
xor(1,0) %异或运算，ans = 1
a=3 % semicolon supressing output 这样可以阻止打印输出
A = [1 2;3 4;4 5] %这样会生成一个三行两列的矩阵A
v = [1 2 3] %这是一个1*3的矩阵
v = [1;2;3] %这是一个3*1的列向量
v = 1:0.1:2 %这是一个1*11的矩阵，是行向量，1是起始点，0.1是步长，2是终点
v = 1:6 %一个1到6的行向量
ones(2,3) %生成一个2*3的，元素都是1的矩阵；zeros(1,3)会生成一个1*3的零矩阵
w = rand(1,3) %生成一个1*3的内容随机的行向量
w = randn(1,3) %生成一个1*3的内容随机但服从高斯分布的行向量
hist(w) %根据w的值画出直方图，hist(w，50)画出的直方图中有50条柱子
eye(4) %生成一个4*4的单位矩阵
```
Octave控制台里输入下面代码可以更改提示符：
PS1('>> ');  

# p27 加载、移动数据
```Octave
size(A) %返回矩阵A的大小，其实也是个行向量
size(A,1) %返回矩阵A的行数大小 ，size(A,2)返回列数
length(A) %返回矩阵维度较大的那个

load featuresX.dat %featuresX.dat里假设有训练数据，这时候我们就有了featuresX这个变量
load('featuresX.dat') %加载数据另一种方法
v = featuresX(1:10) %得到矩阵的前10个
save hello.mat v; %将v变量的内容保存到hello.mat这个文件中
who %显示所有内存中的变量
whos %显示更详细的变量信息
clear xxx %删除xxx这个变量，单独用clear是删除全部变量
A(3,2) %会得到A矩阵的第3行第2个
A(3,:) %会得到A矩阵的第3行所有元素
A([1,3],:) %会得到A矩阵的第1行和第3行所有元素
A(:,2) = [10;11;12] %对A矩阵的第二列进行赋值
A = [A,[10;11;12]] %在A的右侧增加一列
A(:) %将所有元素，变成一个列向量
C = [A;B] %B放到A的下面
```

# p28 计算数据
```Octave
A*B %矩阵相乘
A .* B %A的每个元素和B的每个元素一一对应相乘
A .^2  %A的每个元素进行平方计算
log(A) %对A的每个元素进行对数运算
exp(A) %对A的每个元素进行以e为底，元素为指数的幂运算
abs(A) %求A中所有元素的绝对值
A + 1 %A的所有值都加上1
A'   %得到A的转置矩阵
max(a)  %返回最大的值，当A是矩阵时会返回每一列的最大值
[val,index] = max(a) %得到最大值跟索引
a < 3 %返回一个由0或者1构成的结构，相当于把a的每个元素与3进行比较并得到结果
find(a < 3) %得到满足条件的索引，从1开始数
magic(3) %得到一个矩阵，这个矩阵每一行，每一列，还有对角线上的值，加起来都相同，一般拿来生成随机矩阵用（基本在机器学习中不会用到）
[r,c] = find(A >= 7) %会得到r和c两个列向量，两列向量的元素分别对应表示第几行，第几列的值满足条件
sum(a) %得到向量a的每个元素加起来的值
sum(a,1) %得到一个行向量，是矩阵a的每一列的总和；sum(a,2)就是求每一行的总和，得到的是一个列向量
prod(a) %product，表示a的每个元素相乘的积
floor(a)、ceil(a) %分别是向下取整和向上取整
rand(3) %得到3*3的随机矩阵
max(A,[],1) %得到每一列最大的值
max(A,[],2) %得到每一行最大的值
max(max(A)) %得到矩阵的一个最大值，max(A(:))也可以
A .*eye(9) %得到一个矩阵，除了对角线，其他值都是0
flipud(eye(9)) %得到一个垂直翻转的矩阵
pinv(A) %得到伪逆矩阵
```

# p29 数据绘制
```Octave
t=[0:0.01:0.98];
y1 = sin(2*pi*4*t);
plot(t,y1); %t为x轴的值，y1为y轴的值，画出一个图来
y2 = cos(2*pi*4*t);
plot(t,y2);
%这时候会覆盖之前的图
% 用hold on就不会清空之前的图
plot(t,y1);
hold on;
plot(t,y2,'r'); %给余弦值一个红色
xlabel('time') %给横轴一个标签值
ylabel('value') %同理
legend('sin','cos') %添加图例
title('my plot') %添加标题
print -dpng 'myPlot.png' %将图表保存起来
close %关闭图表
%画两个图并且不会相互影响
figure(1);plot(t,y1)
figure(2);plot(t,y2) 
subplot(1,2,1) %画一个1*2的格子，然后使用第一个格子
plot(t,y1) %这时候图表会画在第一个格子里
subplot(1,2,2) %选择使用第二个格子
plot(t,y2) %这时候图表会画在第二个格子里
axis([0.5 1 -1 1]) %设置前两个设置x轴的范围，后两个设置y轴的范围
clf %清空画布
A = magic(5)
imagesc(A) %变成一个带彩色的矩阵图
imagesc(A),colorbar,colormap gray %一个灰度图，带颜色图例条
%使用","号可以执行多个命令，依次执行
```

# p30控制语句:for,while,if
```Octave
v = zeros(10,1)
for i = 1:10,
    v(i) = 2^i;
end;

indices = 1:10
for i = indices,
    disp(i); %显示i
end;

i = 1;
while i <= 5,
    v(i) = 100;
    i = i + 1;
end;

i = 1;
while true,
    v(i) = 999;
    i = i + 1;
    if i == 6,
        break;%退出
    end;
end;

if v(1) == 1,
    disp('xxx');
elseif v(1) == 2,
    disp('xx)
else
    disp('x')
end;
exit; %退出Octave，quit也行


```

在Octave定义函数，你需要新建一个文件,文件名是：函数名.m
```
function y = squareThissNumber(x)

y = x^2;
```

```
%调用
addpath('c:\xxx') %添加这个路径后，就可以找到这个路径下的函数文件
squareThissNumber(2) %这时候就会输出4
```


也可以定义函数返回两个值
```
function [y1,y2] = squareAndCubeThisNumber(x)

y1 = x^2;
y2 = x^3;
```

# p31 向量化
求和操作可以看作是一个行向量 乘以 列向量
讲解到如何将梯度下降算法向量化

# p32 分类
将讨论logistic regression 逻辑回归算法
对于分类问题，使用线性回归算法是效果差的