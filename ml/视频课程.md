学习笔记，以视频集数为标题，总结每集的收获和知识点

[黑马最热门的人工智能机器学习及机器视觉终于来了！从入门到精通（附赠课件资料+笔记](https://www.bilibili.com/video/BV1wy4y1T78o)
这个视频TensorFlow用的是1.x的版本，从网上资料来看，目前2.x版本中已经废弃了很多1.x的api，并且操作上更加简洁，所以这个视频我先暂时不看了

# p2学习笔记
## 深度学习与机器学习的区别
特征提取方面：
    机器学习需要自己提取特征，这需要大量领域专业知识；
    深度学习（通常由多个层组成），通过大量数据训练自动得到模型，适用于难提取特征的图像，语音，自然语言领域（应用场景）；

数据量方面：
    深度学习一般数据量越大结论越准确，而机器学习在一定数据量后达到一个相对稳定的准确率，并不会随着后面数据量的增大而表现更佳。
    深度学习需要的算力也比较大

算法代表
    机器学习：朴素贝叶斯、决策树
    深度学习：神经网络



个人总结：在容易提取特征的领域，采用机器学习的方法，通过手动提取特征，在效率和效果方面应该是表现不错的。深度学习需要的数据量应该较大，提取模型的成本也比较大。所以不能说 深度学习 就比 机器学习 优

深度学习框架：github start关注度历年来前二分别是TensorFlow，Caffe 



[[中英字幕]吴恩达机器学习系列课程](https://www.bilibili.com/video/BV164411b7dx)

笔记
# p1
机器学习应用领域广泛，如自动驾驶，基因测序，研究人脑思考方式，搜索引擎推荐，图片分类，识别垃圾邮件等

# p2
介绍一般对于机器学习的定义，给一个任务T,通过经验E,得到性能度量P(我认为算做正确的概率)
这课程会教很多不同类型的学习算法，最主要的两类是 监督学习(supervised learning)和无监督学习（其他的经常听到的有，强化学习，推荐系统等）
简单的说 监督学习是我们让机器学会处理某一件事；无监督学习是我们让机器自己学习

# p3 
回归问题，值是连续的
分类问题，值是离散的
总结：通过值去判断要用把问题归为哪一类（进而选择合适的算法）
这些属于监督学习的范围

# p4
将无监督学习
中监督学习的例子中，我们会被告知什么是良性肿瘤，什么是恶性的；
现在我们拿到一推数据，没人告诉我们这些数据的意义，这时候就属于用到无监督学习的范围了
提到 
聚类算法，将数据归为一个簇一个簇的（应用例子：谷歌新闻：每天收集大量新闻并分为各种专题；给一推基因数据让其分类，类型我们事先未知；根据邮件判断哪些人可能是朋友；通过客户数据找到不同的细分市场）
鸡尾酒会算法（应用例子：耳机降噪），实际上一行算法：[W,s,v] = svd((repmat(sum(x.*x,1),size(x,1),1).*x)*x');
svd：singular value decomposition 奇异值分解（线性代数常规函数）


提到Octave、Matlab这样的软件
作者推荐使用Octave，在硅谷很多人会先用Octave建立软件原型，因为在Octave中实现这些学习算法的速度很快，在这个软件中，例如svd函数也内置了，不然如果自己用c++或者java库我们需要自己写很多代码，当在软件里可以运行时，我们再将其迁移到其他环境（java，c++等）下，这样效率更高（根据弹幕有歧义，说现在python可以很方便的现实功能）

# p5
了解监督学习的过程
1. Traning Set训练集 --输入到>> Learning Algorithm学习算法 --得到>> h(hypotheis)假设函数
2. 输入值通过->h->输出预测值
# p6 代价函数
如果假设函数是这个形式： hθ(x) = θ0 + θ1x;
通过找到最小的值：minimize_θ0θ0  J(θ0,θ0) =  (1/2m) * 0Σi=m (h(x^(i)) - y^(i))^2的平方差和来确定参数值 (这个公式算我手打的...可能不好看，我用_标识后面的符号在底部。。。)

这个1/2m是为了开导时抵消掉平方，对优化结果来说无影响（弹幕提供）
（微积分有点忘了，但是这里感悟是用积分方程可以减低计算量）
微积分有公式的，比如基本的导数公式中 x^u = u*x^(u-1)

J(θ0,θ1) 在这里就是代价函数Cost function，或者叫做lost function。这里这个具体的函数叫做squared error function

这个squared error function对大多数回归问题是个合理的选择（常用）

# p7 代价函数2
作者简化了一下函数：hθ(x) = θ1x;然后将x值代进代价函数中，把θ1作为变量
画出了θ1与结果的图像变化图（可以看到变化的规律，嗯...没什么，算是加深一下对公式作用的理解吧）

# p8 代价函数3
假设函数换回这个形式： hθ(x) = θ0 + θ1x;
然后画图

提到 contour plots等高线图
01：34