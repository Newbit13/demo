学习笔记，以视频集数为标题，总结每集的收获和知识点

[[双语字幕]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V)
[【目录】【中文】【deplearning.ai】【吴恩达课后作业目录】](https://blog.csdn.net/u013733326/article/details/79827273)

[markdown常见数学符号和运算](https://blog.csdn.net/qq1195365047/article/details/88616220)

# p1
cnn（convolutional neural network） 卷积神经网络，后面会讨论，主要应用在图像领域
rnn（recurrent neural network）循环神经网络，用于有序列的数据，后面讨论
# p2 神经网络
深度学习指的是训练神经网络

ReLU (rectified linear unit 修正线性单元)

# p3
介绍应用，无人驾驶，自然语言处理，广告投放，图像识别等

对结构化和非结构化（音频，图像，文本）的处理应用

# p4 为什么深度学习会兴起
数据、网络、计算能力等等的规模，在推动深度学习的进步。还有算法的改进对计算效率的提升帮助很大 

# p5
# p6
# p7 二分分类
一些符号说明
# p9 cost function
损失函数针对当个样本

成本函数针对整个训练集

# p10 梯度下降法
导数，即曲线的斜率，这梯度下降中我觉得最重要的是提供一个 正负值，来让权重w（或者说参数θ）添加或者减少（斜率大小影响的是迭代的速度）

只有一个变量的函数求导用d符号（导数）
多个一个变量的函数求导用是∂符号（偏导数）

# p11 导数derivatives
# p12 更多导数的例子
log(a) 求导为 1/a

# p13 computation graph
# p14 derivatives with a computation graph
提到链式法则：变量a的变化，会引起v的变化，v会引起j的变化（这些变量在指视频中的变量：04:22）

$y = log(1-a)$

$y' = \frac{ 1 }{ 1 - a } * (-1)$

# p15
介绍反向传播中dw1，dw2,db怎么计算得到的（使用链式法则）
# p16 m个样本的梯度下降
# p17 向量化 Vectorization
在测试中，向量化计算速度是循环计算的上百倍
# p18 向量化的更多例子
# p19 
# p20 向量化logistic回归的梯度计算
使用python对一次梯度下降的具体实现
# p21 python中的广播
比如用一个矩阵与实数相加，实数会拓展成一个由这个实数组成的矩阵，再进行计算。类似的计算还有矩阵与向量的四则运算
# p22 一些避免写出python bug的建议
1. 进行矩阵计算时，用```np.random.rand(5)```不如用```np.random.rand(5,1)```，因为前者不算个矩阵，在使用```np.dot```时如果不熟悉可能会得到不是自己需要的结果
2. 使用断言，如：```assert( a.shape == (5,1))```
# p23
# p24
# p25 神经网络概览
# p26 神经网络表示
约定一些符号的含义
# p27 神经网络的输出
经验：把同一层中的不同节点纵向的堆叠起来(变成列向量)
4行代码计算出单个样本的神经网络的输出(这里是双层神经网络)

$z^{[1]} = W^{[1]}x + b^{[1]}$

$a^{[1]} = σ(z^{[1]})$

$z^{[2]} = W^{[2]}a^{[1]} + b^{[2]}$

$a^{[2]} = σ(z^{[2]})$

# p28 多个样本的向量化
以样本为列进行横向堆叠
# p29 向量化实现的解释
# p30 激活函数
不只有sigmoid函数(在输出层，如果想要区分0和1的结果，还是会使用的)

tanh函数：y值范围是-1~1，公式 $g(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$

sigmoid和tanh这两激活函数的确定，当函数参数值太大或太小时，其斜率接近与0，导致拖慢梯度下降算法

ReLU：g(z) = max(0,z) ,只要z为正，其导数就是1。根据作者经验，一般隐藏层不知道选什么激活函数就选它。
Leaky ReLU：弥补Rule在z为负时，导数为0的缺点

# p31 为什么需要非线性激活函数
个人理解：线性的话相当于没有，发挥不出神经网络的作用，跟单纯做线性回归的假设没有区别
# p32 激活函数的导数
sigmoid函数导数:a*(1 - a)，其中$a= σ(z) = \frac{1}{1 + e^{-z}}$

tanh(z)求导：1-a^2，其中$a = tanh(z) = \frac{e^z - e^{-z}}{e^z + e^{-z}}$

ReLU函数：g(z) = max(0,z)，

$$ \begin{cases}
g'(z) = 0 & z<0 \\
g'(z) = 1 & z>0 
\end{cases}$$

（等于0时无意义，然后将等于0的导数定位0或者1都没问题）

# p33 神经网络的梯度下降法
正向传播、反向传播的所有公式（重点）
# p34 直观理解反向传播
$dz^{[l]} = a^{[l]} - y$，l代表最后一层

$dz^{[i]} = W^{[i + 1]}.T · dz^{[i + 1]} * g'^{[i]}(z^{[i]})$   （g是隐藏层的激活函数，其斜率参考上一集的公式） （注：我用·表示矩阵相乘，用*表示元素乘元素）

$dw^{[i]} = dz^{[i]} · a^{[i - 1]}$ 向量化多样本表示是：$dW^{[i]} = \frac{1}{m}dZ^{[i]} · A^{[i - 1]}.T$



$db^{[i]} = dz^{[i]}$ 向量化多样本表示是：$db^{[i]} = \frac{1}{m}np.sum(dZ^{[i]},axis = 1,keepdims=True)$

个人想法：以上dw，db是单纯用链式法则对代价函数的推导。在之前的机器学习版本里，通过误差值来解释，算是对这里的数学表达式加上一层直观的理解（给我看来算是变得更加抽象和不好理解了）

# p35 随机初始化
如果w初始化都为0的话，会导致隐藏层的节点值a都相同，进而隐藏层的权重也相同，导致dw的变化也相同，所以需要随机初始化参数

```W1 = np.random.randn((n1,n0)) * 0.01```乘以0.01的原因是如果w太大，导致z太大，进而导致激活函数的斜率太小，最终导致梯度下降算法收敛特别慢（这也解释了为什么特征也要标准化，不能值太大）

```b1 = np.zeros((n0,1))```b相同也没事

# p36 深层神经网络
将前面的双层神经网络拓展为多层神经网络，并给予一些符号表示的约定
# p37 前向传播和反向传播
思路图：08:05
# p38 前向传播
这节展示了用之前的符号约定进行计算，并给出整个训练集向量化的计算公式
# p39 核对矩阵的维数
