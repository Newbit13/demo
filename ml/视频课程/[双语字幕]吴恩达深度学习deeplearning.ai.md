学习笔记，以视频集数为标题，总结每集的收获和知识点

[[双语字幕]吴恩达深度学习deeplearning.ai](https://www.bilibili.com/video/BV1FT4y1E74V)
[【目录】【中文】【deplearning.ai】【吴恩达课后作业目录】](https://blog.csdn.net/u013733326/article/details/79827273)

# p1
cnn（convolutional neural network） 卷积神经网络，后面会讨论，主要应用在图像领域
rnn（recurrent neural network）循环神经网络，用于有序列的数据，后面讨论
# p2 神经网络
深度学习指的是训练神经网络

Relu (rectified linear unit 修正线性单元)

# p3
介绍应用，无人驾驶，自然语言处理，广告投放，图像识别等

对结构化和非结构化（音频，图像，文本）的处理应用

# p4 为什么深度学习会兴起
数据、网络、计算能力等等的规模，在推动深度学习的进步。还有算法的改进对计算效率的提升帮助很大 

# p5
# p6
# p7 二分分类
一些符号说明
# p9 cost function
损失函数针对当个样本

成本函数针对整个训练集

# p10 梯度下降法
导数，即曲线的斜率，这梯度下降中我觉得最重要的是提供一个 正负值，来让权重w（或者说参数θ）添加或者减少（斜率大小影响的是迭代的速度）

只有一个变量的函数求导用d符号（导数）
多个一个变量的函数求导用是∂符号（偏导数）

# p11 导数derivatives
# p12 更多导数的例子
log(a) 求导为 1/a

# p13 computation graph
# p14 derivatives with a computation graph
提到链式法则：变量a的变化，会引起v的变化，v会引起j的变化（这些变量在指视频中的变量：04:22）

y = log(1-a)
y' = 1/(1 - a) * (-1)

# p15
介绍反向传播中dw1，dw2,db怎么计算得到的（使用链式法则）
# p16 m个样本的梯度下降
# p17 向量化 Vectorization
在测试中，向量化计算速度是循环计算的上百倍
# p18 向量化的更多例子
# p19 
# p20 向量化logistic回归的梯度计算
使用python对一次梯度下降的具体实现
# p21 python中的广播
比如用一个矩阵与实数相加，实数会拓展成一个由这个实数组成的矩阵，再进行计算。类似的计算还有矩阵与向量的四则运算
# p22 一些避免写出python bug的建议
1. 进行矩阵计算时，用```np.random.rand(5)```不如用```np.random.rand(5,1)```，因为前者不算个矩阵，在使用```np.dot```时如果不熟悉可能会得到不是自己需要的结果
2. 使用断言，如：```assert( a.shape == (5,1))```
# p23
# p24
# p25 神经网络概览
# p26 神经网络表示
约定一些符号的含义
# p27 神经网络的输出
经验：把同一层中的不同节点纵向的堆叠起来(变成列向量)
4行代码计算出单个样本的神经网络的输出(这里是双层神经网络)
z^[1] = W^[1]x + b^[1]
a^[1] = σ(z^[1])
z^[2] = W^[2]a^[1] + b^[2]
a^[2] = σ(z^[2])
# p28 多个样本的向量化
以样本为列进行横向堆叠
# p29 向量化实现的解释
