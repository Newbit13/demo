大概思路：

    这次分享会主要是向大家分享我这大半个月以来在学习机器学习这个过程中的体会和感受
    虽然我知道我还有很多知识点还没有接触过，但这不妨碍我向大家介绍一些机器学习领域的概念和知识

# 什么是机器学习
机器学习，下面简称ml(machine learning)

    什么是机器学习，在生活中有哪些应用，对应着哪些算法（to do）（目的：引起兴趣，了解全局）

人工智能包括：？？
知识表示
自动推理
搜索方法
机器学习
知识获取
知识处理系统
自然语言理解
计算机视觉
智能机器人
自动程序设计

    专家系统:尽可能模拟专家解决实际问题的决策和工作过程。进行推理和判断，以解决需要人类专家处理的复杂与困难的专业领域问题。
    应用：血液感染病诊断、探矿、青光眼诊断与治疗、模拟人类进行概况，抽象，归纳整理，发现某些数论的概念和定理、语音识别

    专家系统骨架（类比游戏引擎，替换知识库就可以变成不同的专家）
    
    专家系统 = 知识库（？？？） + 推理机（？？？）

[人工智能、机器学习和深度学习的区别？](https://www.zhihu.com/question/57770020)

机器学习是人工智能的一个子集。主要解决分类问题和回归问题，分类问题如识别猫和狗，回归问题如预测房价


深度学习是机器学习中的一种以神经网络为主的一类算法，借助模拟一个类似于人脑机制的网络，实现机器学习要解决的问题。深度学习效果突出，但相比其他算法，缺点是需要大量的数据进行训练

个人对其发展的认知（根据收集的资料，或许不准）：
深度学习->深度神经网络->CNN(用于图像),RNN(用于有时序的比如自然语言、音频等场景，其实一般指的是LSTM,就是在RNN的基础上加上记忆的功能)->transformer(包含两个同样也是用RNN实现的编码器和解码器，加上自注意力机制，在图像识别，音乐生成，股价预测各方面表现优异)->BERT(是一个帮助我们找到词语在文章或者句子中的向量的模型，用于帮助计算机对语言进行处理。它的诞生源于transformer，语言的向量是根据transformer的编码器得到语义确定的，诞生于2018年，在各类NLP比赛中疯狂屠版)->GPT(源于transformer中的解码器，openAi通过大量的训练数据与更多的网络层数，得到了拥有上亿参数的GPT-3，相当于一个包含知识+语境理解+语言组织能力的数据库,可以用于创作音乐，诗歌等。给一个开头就能根据文字的风格不断续写。GPT-3作为商业项目暂无开源，需要在openai的官网申请api进行使用)

## 分类
监督学习、无监督学习

1. 强化学习（是个发展历史不断的大家族）：著名应用：AlphaGo。（算法原理？）在不断的尝试中更新自己的行为准则（那么就有奖励跟惩罚机制）

|通过价值选行为|直接选行为|想象环境并从中学习|
|--|--|--|
|Q learning|Policy Gradients|Model based RL|
|Sarsa|||
|Deep Q Network|||

GAN(对抗生成网络，属于什么？应用是？)：有模块负责生成，有模块负责对想要生成的结果进行评价，然后指导生成模块进行修正



# 入门

    通过一个线性回归的案例，向大家介绍ml中的几个概念： 

    代价函数
    梯度下降算法
    预测函数


    如果时间允许，想要介绍：
    神经网络
    前向后向传播算法

# 实践操作

    介绍完概念，演示一下：用python实现 对比 用tenserflow框架实现，让大家对框架与原生代码之间有个清晰的了解

# 个人学习中的难点与体会

# 讨论与展望未来